---
categories:
  - "[[Networking]]"
topic@networking:
  - "[[Networking General]]"
tags:
created: 2026-01-05T12:46:55+01:00
modified: 2026-01-05T12:52:38+01:00
---
<strong>Web crawler</strong>, sometimes called a <strong>spider</strong> or <strong>spiderbot</strong> and often shortened to <strong>crawler</strong>, <mark style="background: #FFF3A3A6;">is an Internet bot that systematically browses the <strong>World Wide Web</strong></mark> and that <mark style="background: #FFB8EBA6;">is typically operated by search engines for the purpose of <strong>Web indexing</strong></mark>.
 
Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently.

Crawlers consume resources on visited systems and often visit sites unprompted. 
Issues of schedule, load, and "politeness" come into play when large collections of pages are accessed. 
Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a <strong>robots.txt</strong> file can request bots to index only parts of a website, or nothing at all.